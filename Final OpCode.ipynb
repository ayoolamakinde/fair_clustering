{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from input CSV file\n",
      "[]\n",
      "Number of data points: 512\n",
      "Dimension: 10\n",
      "Balance: 4 5\n",
      "Constructing tree...\n",
      "Doing fair clustering...\n",
      "Fairlet decomposition cost: 4472.215748781941\n",
      "Doing k-median clustering on fairlet centers...\n",
      "Computing fair k-median cost...\n",
      "Fairlet decomposition cost: 4472.215748781941\n",
      "k-Median cost: 4952.318329325651\n"
     ]
    }
   ],
   "source": [
    "### The MATLAB module does not work with python 3.7.0. Use 3.6.0 release or older.\n",
    " \n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matlab.engine\n",
    "import time\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    " \n",
    " \n",
    "EPSILON = 0.0001\n",
    " \n",
    "FAIRLETS = []\n",
    "FAIRLET_CENTERS = []\n",
    " \n",
    "class TreeNode:\n",
    " \n",
    "    def __init__(self):\n",
    "        self.children = []\n",
    " \n",
    "    def set_cluster(self, cluster):\n",
    "        self.cluster = cluster\n",
    " \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    " \n",
    "    def populate_colors(self, colors):\n",
    "        \"Populate auxiliary lists of red and blue points for each node, bottom-up\"\n",
    "        self.reds = []\n",
    "        self.blues = []\n",
    "        if len(self.children) == 0:\n",
    "            # Leaf\n",
    "            for i in self.cluster:\n",
    "                if colors[i] == 0:\n",
    "                    self.reds.append(i)\n",
    "                else:\n",
    "                    self.blues.append(i)\n",
    "        else:\n",
    "            # Not a leaf\n",
    "            for child in self.children:\n",
    "                child.populate_colors(colors)\n",
    "                self.reds.extend(child.reds)\n",
    "                self.blues.extend(child.blues)\n",
    " \n",
    " \n",
    "### K-MEDIAN CODE ###\n",
    " \n",
    "def kmedian_cost(points, centroids, dataset):\n",
    "    \"Computes and returns k-median cost for given dataset and centroids\"\n",
    "    return sum(np.amin(np.concatenate([np.linalg.norm(dataset[:,:]-dataset[centroid,:], axis=1).reshape((dataset.shape[0], 1)) for centroid in centroids], axis=1), axis=1))\n",
    "\n",
    "def fair_kmedian_cost(centroids, dataset):\n",
    "    \"Return the fair k-median cost for given centroids and fairlet decomposition\"\n",
    "    total_cost = 0\n",
    "    for i in range(len(FAIRLETS)):\n",
    "        # Choose index of centroid which is closest to the i-th fairlet center\n",
    "        cost_list = [np.linalg.norm(dataset[centroids[j],:]-dataset[FAIRLET_CENTERS[i],:]) for j in range(len(centroids))]\n",
    "        cost, j = min((cost, j) for (j, cost) in enumerate(cost_list))\n",
    "        # Assign all points in i-th fairlet to above centroid and compute cost\n",
    "        total_cost += sum([np.linalg.norm(dataset[centroids[j],:]-dataset[point,:]) for point in FAIRLETS[i]])\n",
    "    return total_cost\n",
    " \n",
    " \n",
    "### FAIRLET DECOMPOSITION CODE ###\n",
    " \n",
    "def balanced(p, q, r, b):\n",
    "    if r==0 and b==0:\n",
    "        return True\n",
    "    if r==0 or b==0:\n",
    "        return False\n",
    "    return min(r*1./b, b*1./r) >= p*1./q\n",
    " \n",
    " \n",
    "def make_fairlet(points, dataset):\n",
    "    \"Adds fairlet to fairlet decomposition, returns median cost\"\n",
    "    FAIRLETS.append(points)\n",
    "    cost_list = [sum([np.linalg.norm(dataset[center,:]-dataset[point,:]) for point in points]) for center in points]\n",
    "    cost, center = min((cost, center) for (center, cost) in enumerate(cost_list))\n",
    "    FAIRLET_CENTERS.append(points[center])\n",
    "    return cost\n",
    " \n",
    " \n",
    "def basic_fairlet_decomposition(p, q, blues, reds, dataset):\n",
    "    \"\"\"\n",
    "    Computes vanilla (p,q)-fairlet decomposition of given points (Lemma 3 in NIPS17 paper).\n",
    "    Returns cost.\n",
    "    Input: Balance parameters p,q which are non-negative integers satisfying p<=q and gcd(p,q)=1.\n",
    "    \"blues\" and \"reds\" are sets of points indices with balance at least p/q.\n",
    "    \"\"\"\n",
    "    assert p <= q, \"Please use balance parameters in the correct order\"\n",
    "    if len(reds) < len(blues):\n",
    "        temp = blues\n",
    "        blues = reds\n",
    "        reds = temp\n",
    "    R = len(reds)\n",
    "    B = len(blues)\n",
    "    assert balanced(p, q, R, B), \"Input sets are unbalanced: \"+str(R)+\",\"+str(B)\n",
    " \n",
    "    if R==0 and B==0:\n",
    "        return 0\n",
    " \n",
    "    b0 = 0\n",
    "    r0 = 0\n",
    "    cost = 0\n",
    "    while (R-r0)-(B-b0) >= q-p and R-r0 >= q and B-b0 >= p:\n",
    "        cost += make_fairlet(reds[r0:r0+q]+blues[b0:b0+p], dataset)\n",
    "        r0 += q\n",
    "        b0 += p\n",
    "    if R-r0 + B-b0 >=1 and R-r0 + B-b0 <= p+q:\n",
    "        cost += make_fairlet(reds[r0:]+blues[b0:], dataset)\n",
    "        r0 = R\n",
    "        b0 = B\n",
    "    elif R-r0 != B-b0 and B-b0 >= p:\n",
    "        cost += make_fairlet(reds[r0:r0+(R-r0)-(B-b0)+p]+blues[b0:b0+p], dataset)\n",
    "        r0 += (R-r0)-(B-b0)+p\n",
    "        b0 += p\n",
    "    assert R-r0 == B-b0, \"Error in computing fairlet decomposition\"\n",
    "    for i in range(R-r0):\n",
    "        cost += make_fairlet([reds[r0+i], blues[b0+i]], dataset)\n",
    "    return cost\n",
    " \n",
    " \n",
    "def node_fairlet_decomposition(p, q, node, dataset, donelist, depth):\n",
    "\n",
    "    # Leaf                                                                                          \n",
    "    if len(node.children) == 0:\n",
    "        node.reds = [i for i in node.reds if donelist[i]==0]\n",
    "        node.blues = [i for i in node.blues if donelist[i]==0]\n",
    "        assert balanced(p, q, len(node.reds), len(node.blues)), \"Reached unbalanced leaf\"\n",
    "        return basic_fairlet_decomposition(p, q, node.blues, node.reds, dataset)\n",
    " \n",
    "    # Preprocess children nodes to get rid of points that have already been clustered\n",
    "    for child in node.children:\n",
    "        child.reds = [i for i in child.reds if donelist[i]==0]\n",
    "        child.blues = [i for i in child.blues if donelist[i]==0]\n",
    " \n",
    "    R = [len(child.reds) for child in node.children]\n",
    "    B = [len(child.blues) for child in node.children]\n",
    " \n",
    "    if sum(R) == 0 or sum(B) == 0:\n",
    "        assert sum(R)==0 and sum(B)==0, \"One color class became empty for this node while the other did not\"\n",
    "        return 0\n",
    " \n",
    "    NR = 0\n",
    "    NB = 0\n",
    " \n",
    "    # Phase 1: Add must-remove nodes\n",
    "    for i in range(len(node.children)):\n",
    "        if R[i] >= B[i]:\n",
    "            must_remove_red = max(0, R[i] - int(np.floor(B[i]*q*1./p)))\n",
    "            R[i] -= must_remove_red\n",
    "            NR += must_remove_red\n",
    "        else:\n",
    "            must_remove_blue = max(0, B[i] - int(np.floor(R[i]*q*1./p)))\n",
    "            B[i] -= must_remove_blue\n",
    "            NB += must_remove_blue\n",
    " \n",
    "    # Calculate how many points need to be added to smaller class until balance\n",
    "    if NR >= NB:\n",
    "        # Number of missing blues in (NR,NB)\n",
    "        missing = max(0, int(np.ceil(NR*p*1./q)) - NB)\n",
    "    else:\n",
    "        # Number of missing reds in (NR,NB)\n",
    "        missing = max(0, int(np.ceil(NB*p*1./q)) - NR)\n",
    "         \n",
    "    # Phase 2: Add may-remove nodes until (NR,NB) is balanced or until no more such nodes\n",
    "    for i in range(len(node.children)):\n",
    "        if missing == 0:\n",
    "            assert balanced(p, q, NR, NB), \"Something went wrong\"\n",
    "            break\n",
    "        if NR >= NB:\n",
    "            may_remove_blue = B[i] - int(np.ceil(R[i]*p*1./q))\n",
    "            remove_blue = min(may_remove_blue, missing)\n",
    "            B[i] -= remove_blue\n",
    "            NB += remove_blue\n",
    "            missing -= remove_blue\n",
    "        else:\n",
    "            may_remove_red = R[i] - int(np.ceil(B[i]*p*1./q))\n",
    "            remove_red = min(may_remove_red, missing)\n",
    "            R[i] -= remove_red\n",
    "            NR += remove_red\n",
    "            missing -= remove_red\n",
    " \n",
    "    # Phase 3: Add unsatuated fairlets until (NR,NB) is balanced\n",
    "    for i in range(len(node.children)):\n",
    "        if balanced(p, q, NR, NB):\n",
    "            break\n",
    "        if R[i] >= B[i]:\n",
    "            num_saturated_fairlets = int(R[i]/q)\n",
    "            excess_red = R[i] - q*num_saturated_fairlets\n",
    "            excess_blue = B[i] - p*num_saturated_fairlets\n",
    "        else:\n",
    "            num_saturated_fairlets = int(B[i]/q)\n",
    "            excess_red = R[i] - p*num_saturated_fairlets\n",
    "            excess_blue = B[i] - q*num_saturated_fairlets\n",
    "        R[i] -= excess_red\n",
    "        NR += excess_red\n",
    "        B[i] -= excess_blue\n",
    "        NB += excess_blue\n",
    " \n",
    "    assert balanced(p, q, NR, NB), \"Constructed node sets are unbalanced\"\n",
    " \n",
    "    reds = []\n",
    "    blues = []\n",
    "    for i in range(len(node.children)):\n",
    "        for j in node.children[i].reds[R[i]:]:\n",
    "            reds.append(j)\n",
    "            donelist[j] = 1\n",
    "        for j in node.children[i].blues[B[i]:]:\n",
    "            blues.append(j)\n",
    "            donelist[j] = 1\n",
    " \n",
    "    assert len(reds)==NR and len(blues)==NB, \"Something went horribly wrong\"\n",
    " \n",
    "    return basic_fairlet_decomposition(p, q, blues, reds, dataset) + sum([node_fairlet_decomposition(p, q, child, dataset, donelist, depth+1) for child in node.children])\n",
    " \n",
    " \n",
    "def tree_fairlet_decomposition(p, q, root, dataset, colors):\n",
    "    \"Main fairlet clustering function, returns cost wrt original metric (not tree metric)\"\n",
    "    assert p <= q, \"Please use balance parameters in the correct order\"\n",
    "    root.populate_colors(colors)\n",
    "    assert balanced(p, q, len(root.reds), len(root.blues)), \"Dataset is unbalanced\"\n",
    "    root.populate_colors(colors)\n",
    "    donelist = [0] * dataset.shape[0]\n",
    "    return node_fairlet_decomposition(p, q, root, dataset, donelist, 0)\n",
    " \n",
    " \n",
    "### QUADTREE CODE ###\n",
    " \n",
    "def build_quadtree(dataset, max_levels=0, random_shift=True):\n",
    "    \"If max_levels=0 there no level limit, quadtree will partition until all clusters are singletons\"\n",
    "    dimension = dataset.shape[1]\n",
    "    lower = np.amin(dataset, axis=0)\n",
    "    upper = np.amax(dataset, axis=0)\n",
    " \n",
    "    shift = np.zeros(dimension)\n",
    "    if random_shift:\n",
    "        for d in range(dimension):\n",
    "            spread = upper[d] - lower[d]\n",
    "            shift[d] = np.random.uniform(0, spread)\n",
    "            upper[d] += spread\n",
    " \n",
    "    return build_quadtree_aux(dataset, range(dataset.shape[0]), lower, upper, max_levels, shift)\n",
    "     \n",
    " \n",
    "def build_quadtree_aux(dataset, cluster, lower, upper, max_levels, shift):\n",
    "    \"\"\"\n",
    "    \"lower\" is the \"bottom-left\" (in all dimensions) corner of current hypercube\n",
    "    \"upper\" is the \"upper-right\" (in all dimensions) corner of current hypercube\n",
    "    \"\"\"\n",
    " \n",
    "    dimension = dataset.shape[1]\n",
    "    cell_too_small = True\n",
    "    for i in range(dimension):\n",
    "        if upper[i]-lower[i] > EPSILON:\n",
    "            cell_too_small = False\n",
    " \n",
    "    node = TreeNode()\n",
    "    if max_levels==1 or len(cluster)<=1 or cell_too_small:\n",
    "        # Leaf\n",
    "        node.set_cluster(cluster)\n",
    "        return node\n",
    "     \n",
    "    # Non-leaf\n",
    "    midpoint = 0.5 * (lower + upper)\n",
    "    subclusters = defaultdict(list)\n",
    "    for i in cluster:\n",
    "        subclusters[tuple([dataset[i,d]+shift[d]<=midpoint[d] for d in range(dimension)])].append(i)\n",
    "    for edge, subcluster in subclusters.items():\n",
    "        sub_lower = np.zeros(dimension)\n",
    "        sub_upper = np.zeros(dimension)\n",
    "        for d in range(dimension):\n",
    "            if edge[d]:\n",
    "                sub_lower[d] = lower[d]\n",
    "                sub_upper[d] = midpoint[d]\n",
    "            else:\n",
    "                sub_lower[d] = midpoint[d]\n",
    "                sub_upper[d] = upper[d]\n",
    "        node.add_child(build_quadtree_aux(dataset, subcluster, sub_lower, sub_upper, max_levels-1, shift))\n",
    "    return node\n",
    " \n",
    "\n",
    "### MAIN ###\n",
    "#sys.argv.append(5)\n",
    "#sys.argv.append(7)\n",
    "#sys.argv.append(20)\n",
    "#datasize = [100,200,300,400,500,600]\n",
    "times = list()\n",
    "#for x in datasize:\n",
    "    \n",
    "start_time = time.clock()\n",
    "sys.argv[1]=4\n",
    "sys.argv[2]=5\n",
    "#sys.argv[3]=\n",
    "sys.argv.append(2)\n",
    "sys.argv[4] = \"Opcode Dataset.csv\"\n",
    "\n",
    "if len(sys.argv) < 4:\n",
    "    print(\"Usage:\")\n",
    "    print(\"First and second parameters are non-negative coprime integers that specify the target balance\")\n",
    "    print(\"Third parameter is k for k-clustering\")\n",
    "    print(\"Fourth parameters is a file in CSV format, where each line is a data point, the first column is 0/1 specifying the colors, and the rest of the columns are numerical specifying the point coordinates.\")\n",
    "    print(\"Fifth parameter is optional, non-negative integer to determine sample size. If given, that number of points will be randomly sampled from the dataset. If not given, will run on the whole dataset.\")\n",
    "    print(\"Terminating\")\n",
    "    sys.exit(0)\n",
    "\n",
    "try:\n",
    "    p = min(int(sys.argv[1]), int(sys.argv[2]))\n",
    "    q = max(int(sys.argv[1]), int(sys.argv[2]))\n",
    "except:\n",
    "    print(\"First two parameters must be non-negative integers that specify the target balance; terminating\")\n",
    "    sys.exit(0)\n",
    "\n",
    "k = int(sys.argv[3])\n",
    "\n",
    "if len(sys.argv) > 4:\n",
    "    # Parse input file in CSV format, first column is colors, other columns are coordinates\n",
    "    print(\"Loading data from input CSV file\")\n",
    "    input_csv_filename = sys.argv[4]\n",
    "    colors = []\n",
    "    points = []\n",
    "    print(colors)\n",
    "    i = 0\n",
    "    skipped_lines = 0\n",
    "    for line in open(input_csv_filename).readlines():\n",
    "        if len(line.strip()) == 0:\n",
    "            skipped_lines += 1\n",
    "            continue\n",
    "        tokens = line[:-1].split(\",\")\n",
    "        try:\n",
    "            color = int(tokens[0])\n",
    "        except:\n",
    "            print(\"Invalid color label in line\", i, \", skipping\")\n",
    "            skipped_lines += 1\n",
    "            continue\n",
    "        try:\n",
    "            point = [float(x) for x in tokens[1:]]\n",
    "        except:\n",
    "            print(\"Invalid point coordinates in line\", i, \", skipping\")\n",
    "            skipped_lines += 1\n",
    "            continue\n",
    "        colors.append(color)\n",
    "        points.append(point)\n",
    "        i += 1\n",
    "\n",
    "    n_points = len(points)\n",
    "    if  n_points == 0:\n",
    "        print(\"No successfully parsed points in input file, terminating\")\n",
    "        sys.exit(0)\n",
    "    dimension = len(points[0])\n",
    "\n",
    "    dataset = np.zeros((n_points, dimension))\n",
    "    for i in range(n_points):\n",
    "        if len(points[i]) < dimension:\n",
    "            print(\"Insufficient dimension in line\", i+skipped_lines, \", terminating\")\n",
    "            sys.exit(0)\n",
    "        for j in range(dimension):\n",
    "            dataset[i,j] = points[i][j]\n",
    "\n",
    "else:\n",
    "    print(\"No input file given; randomizing data\")\n",
    "    n_points = 1000\n",
    "    dimension = 5\n",
    "    dataset = np.random.random((n_points, dimension))\n",
    "    colors = [np.random.randint(2) for i in range(n_points)]\n",
    "\n",
    "if len(sys.argv) > 5:\n",
    "    sample_size = int(sys.argv[5])\n",
    "    idx = np.arange(n_points)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:sample_size]\n",
    "    n_points = sample_size\n",
    "    dataset = dataset[idx,:]\n",
    "    colors = [colors[i] for i in idx]\n",
    "\n",
    "print(\"Number of data points:\", n_points)\n",
    "print(\"Dimension:\", dimension)\n",
    "print(\"Balance:\", p, q)\n",
    "\n",
    "print(\"Constructing tree...\")\n",
    "fairlet_s = time.time()\n",
    "root = build_quadtree(dataset)\n",
    "\n",
    "print(\"Doing fair clustering...\")\n",
    "cost = tree_fairlet_decomposition(p, q, root, dataset, colors)\n",
    "fairlet_e = time.time()\n",
    "\n",
    "print(\"Fairlet decomposition cost:\", cost)\n",
    "\n",
    "print(\"Doing k-median clustering on fairlet centers...\")\n",
    "fairlet_center_idx = [dataset[index] for index in FAIRLET_CENTERS]\n",
    "fairlet_center_pt = np.array([np.array(xi) for xi in fairlet_center_idx])\n",
    "\n",
    "# convert points into matlab array format\n",
    "mat_matrix = matlab.double(fairlet_center_pt.tolist())\n",
    "\n",
    "#print(C)\n",
    "\n",
    "# Run k-mediod code in Matlab\n",
    "cluster_s = time.time()\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "\n",
    "# C: Cluster medoid locations, returned as a numeric matrix.\n",
    "# C is a k-by-d matrix, where row j is the medoid of cluster j\n",
    "#\n",
    "# midx: Index to mat_matrix, returned as a column vector of indices.\n",
    "# midx is a k-by-1 vector and the indices satisfy C = X(midx,:)\n",
    "idx,C,sumd,D,midx,info = eng.kmedoids(matlab.double(dataset.tolist()), k,'Distance','euclidean', nargout=6)\n",
    "#cluster_e = time.time()\n",
    "\n",
    "#np_idx = (np.array(idx._data)).flatten()\n",
    "\n",
    "\n",
    "#np_idx = (np.array(idx._data)).flatten()\n",
    "\n",
    "# compute the indices of centers returned by Matlab in its input matrix\n",
    "\n",
    "# which is mat_matrix or fairlet_center_pt\n",
    "\n",
    "idx= (np.array(idx).flatten())\n",
    "idx = idx.astype(int)\n",
    "#in matlab, arrays are numbered from 1\n",
    "id_x = [index - 1 for index in idx]\n",
    "\n",
    "#convert predicted label from idx variable\n",
    "id_x = np.where(id_x==1, 0, id_x)\n",
    "id_x = np.where(id_x==2, 1, id_x)\n",
    "\n",
    "            \n",
    "idx,C,sumd,D,midx,info = eng.kmedoids(mat_matrix, k,'Distance','euclidean', nargout=6)\n",
    "\n",
    "#indices of center points in dataset\n",
    "\n",
    "np_midx = (np.array(midx._data)).flatten()\n",
    "c_idx_matrix = np_midx.astype(int)\n",
    "c_idx_matrix[:] = [index - 1 for index in c_idx_matrix]\n",
    "centroids = [FAIRLET_CENTERS[index] for index in c_idx_matrix]\n",
    "\n",
    "print(\"Computing fair k-median cost...\")\n",
    "kmedian_cost = fair_kmedian_cost(centroids, dataset)\n",
    "print(\"Fairlet decomposition cost:\", cost)\n",
    "print(\"k-Median cost:\", kmedian_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       268\n",
      "           1       0.79      0.79      0.79       244\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       512\n",
      "   macro avg       0.80      0.80      0.80       512\n",
      "weighted avg       0.80      0.80      0.80       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "print(\"classification Report\\n\",classification_report(colors,id_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.796875\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(colors,id_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      " [[216  52]\n",
      " [ 52 192]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix\\n\",confusion_matrix(colors,id_x))\n",
    "cm = confusion_matrix(colors,id_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
